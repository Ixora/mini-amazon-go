{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 训练你的物体检测器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gluoncv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gluoncv as gcv\n",
    "import mxnet as mx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 准备训练集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "class DetectionDataset(gcv.data.VOCDetection):\n",
    "    CLASSES = ['cocacola', 'noodles', 'hand', 'fake']  # , 'cocacola-zero', 'juice'  # yolo3 need at least 4 classes!!!\n",
    "    def __init__(self, root):\n",
    "        self._im_shapes = {}\n",
    "        self._root = os.path.expanduser(root)\n",
    "        self._transform = None\n",
    "        self._items = [(self._root, x.strip('.xml')) for x in os.listdir(self._root) if x.endswith('.xml')]\n",
    "        self._anno_path = os.path.join('{}', '{}.xml')\n",
    "        self._image_path = os.path.join('{}', '{}.jpg')\n",
    "        self.index_map = dict(zip(self.classes, range(self.num_class)))\n",
    "        self._label_cache = self._preload_labels()\n",
    "        \n",
    "    def __str__(self):\n",
    "        detail = self._root\n",
    "        return self.__class__.__name__ + '(' + detail + ')'\n",
    "    \n",
    "    @property\n",
    "    def classes(self):\n",
    "        return self.CLASSES\n",
    "    \n",
    "    @property\n",
    "    def num_class(self):\n",
    "        return len(self.classes)\n",
    "        \n",
    "train_dataset = DetectionDataset('../images/china-v1')\n",
    "print('class_names:', train_dataset.classes)\n",
    "print('num_images:', len(train_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 可视化数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from gluoncv.utils import viz\n",
    "\n",
    "sample = train_dataset[0]\n",
    "train_image = sample[0]\n",
    "train_label = sample[1]\n",
    "\n",
    "ax = viz.plot_bbox(\n",
    "    train_image.asnumpy(),\n",
    "    train_label[:, :4],\n",
    "    labels=train_label[:, 4:5],\n",
    "    class_names=train_dataset.classes)\n",
    "plt.show()\n",
    "\n",
    "# for i in range(len(train_dataset)):\n",
    "#     sample = train_dataset[i]\n",
    "#     train_image = sample[0]\n",
    "#     train_label = sample[1]\n",
    "\n",
    "#     ax = viz.plot_bbox(\n",
    "#         train_image.asnumpy(),\n",
    "#         train_label[:, :4],\n",
    "#         labels=train_label[:, 4:5],\n",
    "#         class_names=train_dataset.classes)\n",
    "#     plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 定义训练过程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from datetime import datetime\n",
    "from mxnet import autograd\n",
    "from gluoncv.data.batchify import Tuple, Stack, Pad\n",
    "\n",
    "def train_model(train_dataset, epochs=50):\n",
    "    ctx = mx.gpu(0)\n",
    "#     ctx = mx.cpu(0)\n",
    "    net = gcv.model_zoo.get_model('yolo3_darknet53_custom', classes=train_dataset.classes, transfer='coco')\n",
    "    #net.load_parameters('object_detector_epoch100_10_23_2019_21_59_13.params')  # TODO continue training\n",
    "    net.collect_params().reset_ctx(ctx)\n",
    "    width, height = 512, 512  # suppose we use 512 as base training size\n",
    "    gcv.utils.random.seed(233)\n",
    "    \n",
    "    batch_size = 16  # 16 for p3.2xlarge, 8 for p2.2xlarge\n",
    "    # you can make it larger(if your CPU has more cores) to accelerate data loading\n",
    "    num_workers = 4\n",
    "\n",
    "    train_transform = gcv.data.transforms.presets.yolo.YOLO3DefaultTrainTransform(width, height, net)\n",
    "    batchify_fn = Tuple(*([Stack() for _ in range(6)] + [Pad(axis=0, pad_val=-1) for _ in range(1)]))\n",
    "    train_loader = mx.gluon.data.DataLoader(\n",
    "        train_dataset.transform(train_transform),\n",
    "        batch_size,\n",
    "        shuffle=True,\n",
    "        batchify_fn=batchify_fn,\n",
    "        last_batch='rollover',\n",
    "        num_workers=num_workers)\n",
    "    \n",
    "    sigmoid_ce = mx.gluon.loss.SigmoidBinaryCrossEntropyLoss(from_sigmoid=False)\n",
    "    l1_loss = mx.gluon.loss.L1Loss()\n",
    "    obj_metrics = mx.metric.Loss('ObjLoss')\n",
    "    center_metrics = mx.metric.Loss('BoxCenterLoss')\n",
    "    scale_metrics = mx.metric.Loss('BoxScaleLoss')\n",
    "    cls_metrics = mx.metric.Loss('ClassLoss')\n",
    "    for k, v in net.collect_params().items():\n",
    "        if 'yolodetectionblock' not in k and 'yolooutput' not in k:\n",
    "            # freeze upper layers\n",
    "            v.grad_req = 'null'\n",
    "    trainer = mx.gluon.Trainer(\n",
    "        net.collect_params(), 'sgd',\n",
    "        {'learning_rate': 0.0005, 'wd': 0.0005, 'momentum': 0.9})  # 'learning_rate': 0.001\n",
    "    \n",
    "    net.hybridize(static_alloc=True, static_shape=True)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        tic = time.time()\n",
    "        btic = time.time()\n",
    "        \n",
    "        for i, batch in enumerate(train_loader):\n",
    "            batch_size = batch[0].shape[0]\n",
    "            data = mx.gluon.utils.split_and_load(batch[0], ctx_list=[ctx], batch_axis=0)\n",
    "            # objectness, center_targets, scale_targets, weights, class_targets\n",
    "            fixed_targets = [mx.gluon.utils.split_and_load(batch[it], ctx_list=[ctx], batch_axis=0) for it in range(1, 6)]\n",
    "            gt_boxes = mx.gluon.utils.split_and_load(batch[6], ctx_list=[ctx], batch_axis=0)\n",
    "            sum_losses = []\n",
    "            obj_losses = []\n",
    "            center_losses = []\n",
    "            scale_losses = []\n",
    "            cls_losses = []\n",
    "            \n",
    "            with autograd.record():\n",
    "                for ix, x in enumerate(data):\n",
    "                    obj_loss, center_loss, scale_loss, cls_loss = net(x, gt_boxes[ix], *[ft[ix] for ft in fixed_targets])\n",
    "                    sum_losses.append(obj_loss + center_loss + scale_loss + cls_loss)\n",
    "                    obj_losses.append(obj_loss)\n",
    "                    center_losses.append(center_loss)\n",
    "                    scale_losses.append(scale_loss)\n",
    "                    cls_losses.append(cls_loss)\n",
    "                autograd.backward(sum_losses)\n",
    "            trainer.step(batch_size)\n",
    "            obj_metrics.update(0, obj_losses)\n",
    "            center_metrics.update(0, center_losses)\n",
    "            scale_metrics.update(0, scale_losses)\n",
    "            cls_metrics.update(0, cls_losses)\n",
    "            name1, loss1 = obj_metrics.get()\n",
    "            name2, loss2 = center_metrics.get()\n",
    "            name3, loss3 = scale_metrics.get()\n",
    "            name4, loss4 = cls_metrics.get()\n",
    "            print('[Epoch {}][Batch {}], LR: {:.2E}, Speed: {:.3f} samples/sec, {}={:.3f}, {}={:.3f}, {}={:.3f}, {}={:.3f}'.format(\n",
    "                epoch, i, trainer.learning_rate, batch_size/(time.time()-btic), name1, loss1, name2, loss2, name3, loss3, name4, loss4))\n",
    "            btic = time.time()\n",
    "    return net\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 开始训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "epochs = 200\n",
    "net = train_model(train_dataset, epochs=epochs)\n",
    "save_file = 'object_detector_epoch{}_{}.params'.format(epochs, datetime.now().strftime(\"%m_%d_%Y_%H_%M_%S\"))\n",
    "net.save_parameters(save_file)\n",
    "print('Saved model to disk: ' + save_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_mxnet_p36",
   "language": "python",
   "name": "conda_mxnet_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
